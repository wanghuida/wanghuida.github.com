<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Java | 惠达的小宅]]></title>
  <link href="http://www.wanghd.com/blog/categories/java/atom.xml" rel="self"/>
  <link href="http://www.wanghd.com/"/>
  <updated>2012-11-09T17:42:31+08:00</updated>
  <id>http://www.wanghd.com/</id>
  <author>
    <name><![CDATA[王惠达 huida wanghuida]]></name>
    <email><![CDATA[wanghuida258@yahoo.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[编写hadoop的Map-Reduce]]></title>
    <link href="http://www.wanghd.com/blog/2012/11/04/bian-xie-hadoopde-map-reduce/"/>
    <updated>2012-11-04T21:12:00+08:00</updated>
    <id>http://www.wanghd.com/blog/2012/11/04/bian-xie-hadoopde-map-reduce</id>
    <content type="html"><![CDATA[<h3>创建map-reduce项目</h3>

<ul>
<li>安装hadoop请参考<a href="/blog/2012/11/02/an-zhuang-pei-zhi-hadoop/">这篇文章</a></li>
<li>安装hadoop-plugin请参考<a href="/blog/2012/11/03/eclipsepei-zhi-hadoopde-map-reducekai-fa-huan-jing/">这篇文章</a></li>
</ul>


<p><img src="/images/post/new-mapred.png" title="new-mapred" alt="new-mapred" /></p>

<h3>创建测试数据</h3>

<p>```
cd /tmp
mkdir input
echo "hello world baby huida" > t1.txt
echo "world baby baby markdown" > t2.txt</p>

<p>bin/hadoop fs -put /tmp/input input
bin/hadoop fs -ls
```</p>

<!-- more -->


<h3>复制测试代码</h3>

<ul>
<li>hadoop源代码里的例子：/src/example/org/apache/hadoop/example/WordCount.java</li>
</ul>


<p>```java
package org.anjuke.mapred;</p>

<p>import java.io.IOException;
import java.util.StringTokenizer;</p>

<p>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;</p>

<p>public class WordCount {</p>

<p>  public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable>{</p>

<pre><code>private final static IntWritable one = new IntWritable(1);
private Text word = new Text();

public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
  StringTokenizer itr = new StringTokenizer(value.toString());
  while (itr.hasMoreTokens()) {
    word.set(itr.nextToken());
    context.write(word, one);
  }
}
</code></pre>

<p>  }</p>

<p>  public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable> {</p>

<pre><code>private IntWritable result = new IntWritable();

public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
  int sum = 0;
  for (IntWritable val : values) {
    sum += val.get();
  }
  result.set(sum);
  context.write(key, result);
}
</code></pre>

<p>  }</p>

<p>  public static void main(String[] args) throws Exception {</p>

<pre><code>Configuration conf = new Configuration();
String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
if (otherArgs.length != 2) {
  System.err.println("Usage: wordcount &lt;in&gt; &lt;out&gt;");
  System.exit(2);
}
Job job = new Job(conf, "word count");
job.setJarByClass(WordCount.class);
job.setMapperClass(TokenizerMapper.class);
job.setCombinerClass(IntSumReducer.class);
job.setReducerClass(IntSumReducer.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
System.exit(job.waitForCompletion(true) ? 0 : 1);
</code></pre>

<p>  }
}
```</p>

<h3>运行实例</h3>

<ul>
<li>修改运行参数，根据自己的路径修改</li>
</ul>


<p><img src="/images/post/wordcount-conf.png" title="wordcount-conf" alt="wordcount-conf" /></p>

<ul>
<li>点击运行后看结果</li>
</ul>


<p><img src="/images/post/mapred-run.png" title="mapred-run" alt="mapred-run" /></p>

<p><img src="/images/post/wordcount-ret.png" title="wordcount-ret" alt="wordcount-ret" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[eclipse配置hadoop的map-reduce开发环境]]></title>
    <link href="http://www.wanghd.com/blog/2012/11/03/eclipsepei-zhi-hadoopde-map-reducekai-fa-huan-jing/"/>
    <updated>2012-11-03T00:14:00+08:00</updated>
    <id>http://www.wanghd.com/blog/2012/11/03/eclipsepei-zhi-hadoopde-map-reducekai-fa-huan-jing</id>
    <content type="html"><![CDATA[<h3>生成eclipse环境</h3>

<ul>
<li>安装hadoop请参考<a href="/blog/2012/11/02/an-zhuang-pei-zhi-hadoop/">这篇文章</a></li>
</ul>


<p>```
cd hadoop-1.0.4</p>

<h1>肯定会遇到诸多问题</h1>

<p>ant eclipse
```</p>

<blockquote><p>问题1：configure: error: C++ preprocessor "/lib/cpp" fails sanity check</p>

<blockquote><p>mac没有安装Xcode的command tool</p></blockquote>

<p>问题2：can't exec "glibtoolize"</p>

<blockquote><p>brew install autoconf automake libtool</p></blockquote>

<p>问题3: .eclipse.templates does not exist</p>

<blockquote><p>自己在hadoop-1.0.4下创建一个mkdir .eclipse.templates</p></blockquote></blockquote>

<!-- more -->


<h3>编译hadoop-eclipse-plugin</h3>

<br />


<p>1.进入eclipse插件目录</p>

<p><code>
cd src/contrib/eclipse-plugin
</code></p>

<p>2.编辑build.properties</p>

<p>```
vim build.properties</p>

<h1>设置属性，eclipse.home根据自己的定义</h1>

<p>eclipse.home = /Applications/eclipse
version = 1.0.4
```</p>

<p>3.编辑build.xml</p>

<p>```</p>

<h1>在target=jar里面增加下面的jar包</h1>

<p><copy file="${hadoop.root}/lib/commons-lang-2.4.jar"  todir="${build.dir}/lib" verbose="true"/>
<copy file="${hadoop.root}/lib/commons-configuration-1.6.jar"  todir="${build.dir}/lib" verbose="true"/>
<copy file="${hadoop.root}/lib/jackson-core-asl-1.8.8.jar"  todir="${build.dir}/lib" verbose="true"/>
<copy file="${hadoop.root}/lib/jackson-mapper-asl-1.8.8.jar"  todir="${build.dir}/lib" verbose="true"/>
<copy file="${hadoop.root}/lib/commons-httpclient-3.0.1.jar"  todir="${build.dir}/lib" verbose="true"/>
```</p>

<p>4.编辑vim META-INF/MANIFEST.MF</p>

<p>```</p>

<h1>修改Bundle-ClassPath</h1>

<p>Bundle-ClassPath: classes/,
  lib/hadoop-core.jar,
  lib/commons-configuration-1.6.jar,
  lib/commons-httpclient-3.0.1.jar,
  lib/commons-lang-2.4.jar,
  lib/jackson-core-asl-1.8.8.jar,
  lib/jackson-mapper-asl-1.8.8.jar</p>

<p>```</p>

<p>5.复制hadoop核心包, 并生成我们需要的插件</p>

<p>```
cp {root}/hadoop-core-1.0.4.jar {root}/build</p>

<h1>生成jar包</h1>

<p>ant jar
```</p>

<p>6.编译成功后在build/contrib/eclipse-plugin下找到hadoop-eclipse-plugin-1.0.4.jar</p>

<h3>eclipse配置</h3>

<ul>
<li><p>复制hadoop-eclipse-plugin-1.0.4.jar到eclipse的plugins下，然后重启eclipse</p></li>
<li><p>Preferences => Hadoop Map/Reduce</p></li>
</ul>


<p><img src="/images/post/hadoop-eclipse-location.png" title="hadoop-eclipse-location.png" alt="hadoop-eclipse-location.png" /></p>

<ul>
<li>Window => Show View => Other... => Map/Reduce Locations => New Hadoop Location...</li>
</ul>


<p><img src="/images/post/hadoop-eclipse-general.png" title="hadoop-eclipse-general.png" alt="hadoop-eclipse-general.png" /></p>

<ul>
<li>看下结果吧</li>
</ul>


<p><img src="/images/post/hadoop-eclipse-result.png" title="hadoop-eclipse-result.png" alt="hadoop-eclipse-result.png" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装配置hadoop]]></title>
    <link href="http://www.wanghd.com/blog/2012/11/02/an-zhuang-pei-zhi-hadoop/"/>
    <updated>2012-11-02T23:38:00+08:00</updated>
    <id>http://www.wanghd.com/blog/2012/11/02/an-zhuang-pei-zhi-hadoop</id>
    <content type="html"><![CDATA[<h3>下载stable的hadoop</h3>

<ul>
<li>hadoop的版本比较多，建议下载<code>stable</code>的，现在是<code>hadoop-1.0.4</code></li>
<li>提供一个<a href="http://hadoop.apache.org/releases.html#Download">下载链接</a></li>
</ul>


<h3>解压缩</h3>

<p><code>
tar -zxvf hadoop-1.0.4.tar.gz
cd hadoop-1.0.4
</code></p>

<h3>配置hadoop</h3>

<blockquote><p>vim编辑conf/hadoop-env.sh，设置环境变量</p></blockquote>

<p>```</p>

<h1>如果是mac的话，请添加下面这句</h1>

<p>export HADOOP_OPTS="-Djava.security.krb5.realm=OX.AC.UK</p>

<pre><code>-Djava.security.krb5.kdc=kdc0.ox.ac.uk:kdc1.ox.ac.uk"
</code></pre>

<h1>配置JDK，mac的关系所以路径比较飘逸</h1>

<p>export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK/Home
```</p>

<!-- more -->


<blockquote><p>vim编译conf/core-site.xml，设置hdfs端口</p></blockquote>

<p>```
<configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<blockquote><p>vim编辑conf/hdfs-site.xml，设置hdfs复制份数</p></blockquote>

<p>```
<configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;  
    &lt;value&gt;1&lt;/value&gt;  
&lt;/property&gt; 
</code></pre>

<p></configuration>
```</p>

<blockquote><p>vim编译conf/mapred-site.xml，设置job跟踪器的端口(map-reduce的)</p></blockquote>

<p>```
<configuration></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;localhost:9001&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p></configuration>
```</p>

<h3>配置ssh</h3>

<p><code>
cat ~/.ssh/id_dsa.pub &gt;&gt; authorized_keys
</code></p>

<p><img src="/images/post/ssh.jpg" title="ssh" alt="ssh" /></p>

<h3>启动测试hodoop</h3>

<ul>
<li>如果端口没有被占用的应该一切OK了</li>
</ul>


<p>```
bin/hadoop namenode -format
bin/start-all.sh</p>

<h1>增加一些文件，这里用你自己的目录替换</h1>

<p>bin/hadoop fs -put ~/project/mogilefs-tool /mogilefs-tool
```</p>

<ul>
<li>在游览器里查看下结果，输入localhost:50070</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[显示solr省略的日志]]></title>
    <link href="http://www.wanghd.com/blog/2012/09/27/solrsheng-lue-ri-zhi/"/>
    <updated>2012-09-27T13:54:00+08:00</updated>
    <id>http://www.wanghd.com/blog/2012/09/27/solrsheng-lue-ri-zhi</id>
    <content type="html"><![CDATA[<h3>solr日志默认的效果</h3>

<ul>
<li>删除时默认只显示10条记录，其余的使用省略号代替</li>
<li>一般情况下这样是比较好的，但是有时还是希望看到所有的信息</li>
</ul>


<p><code>
webapp=/solr path=/update params={wt=json}
{delete=[11, 12, 13, 14, 15, 16, 17, 18, 19, 20... (100 deletes)]} 0 127
</code></p>

<!--more -->


<h3>测试程序</h3>

<ul>
<li>查询100条记录后删除</li>
</ul>


<p>```php
&lt;?php
include_once('Apache/Solr/Service.php');
$solr = new Apache_Solr_Service('localhost','8984','/solr');</p>

<p>$rep = $solr->search('<em>:</em>',0,100);
$rep = $rep->response->docs;</p>

<p>$ids = array();
foreach($rep as $r){</p>

<pre><code>var_dump($r-&gt;id);
$ids[] = $r-&gt;id;
</code></pre>

<p>}
$solr->deleteByMultipleIds($ids);
```</p>

<h3>跟踪solr代码</h3>

<ul>
<li>断点设置在LogUpdateProcessor的processDelete里</li>
<li>小于maxNumToLog才会添加到deletes</li>
<li>多余的部分用省略号表示</li>
</ul>


<p>```java
  @Override
  public void processDelete( DeleteUpdateCommand cmd ) throws IOException {</p>

<pre><code>if (logDebug) { log.debug("PRE_UPDATE " + cmd.toString()); }
if (next != null) next.processDelete(cmd);

if (cmd.isDeleteById()) {
  if (deletes == null) {
    deletes = new ArrayList&lt;String&gt;();
    toLog.add("delete",deletes);
  }
  if (deletes.size() &lt; maxNumToLog) { //小于maxNumToLog才会添加到deletes
    long version = cmd.getVersion();
    String msg = cmd.getId();
    if (version != 0) msg = msg + " (" + version + ')';
    deletes.add(msg);
  }
} else {
  if (toLog.size() &lt; maxNumToLog) {
    long version = cmd.getVersion();
    String msg = cmd.query;
    if (version != 0) msg = msg + " (" + version + ')';
    toLog.add("deleteByQuery", msg);
  }
}
numDeletes++;
</code></pre>

<p>  }</p>

<p>  @Override
  public void finish() throws IOException {</p>

<pre><code>//略
if (deletes != null &amp;&amp; numDeletes &gt; maxNumToLog) {
  deletes.add("... (" + numDeletes + " deletes)"); //如果大于就要省略号表示
}
//略
</code></pre>

<p>  }
```</p>

<h3>更改配置，显示所有结果</h3>

<ul>
<li>默认配置中是没有updateRequestProcessorChain，如果没有配置，默认也会加载下面3个处理器</li>
<li>RunUpdateProcessorFactory一般必须要有的，作用是更新请求</li>
<li>DistributedUpdateProcessorFactory作用是更新shard</li>
<li>LogUpdateProcessorFactory作用是记录更新日志</li>
<li>maxNumToLog决定日志显示的个数，默认为10</li>
<li>参考<a href="http://wiki.apache.org/solr/UpdateRequestProcessor">http://wiki.apache.org/solr/UpdateRequestProcessor</a></li>
<li>参考<a href="http://wiki.apache.org/solr/SolrConfigXml">http://wiki.apache.org/solr/SolrConfigXml</a></li>
</ul>


<p>```
  <updateRequestProcessorChain name="" default="true" ></p>

<pre><code>&lt;processor class="solr.DistributedUpdateProcessorFactory" /&gt;
&lt;processor class="solr.LogUpdateProcessorFactory" &gt;
  &lt;int name="maxNumToLog"&gt;100&lt;/int&gt;
&lt;/processor&gt;
&lt;processor class="solr.RunUpdateProcessorFactory" /&gt;
</code></pre>

<p>  </updateRequestProcessorChain>
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[solr的数值存储和范围查询]]></title>
    <link href="http://www.wanghd.com/blog/2012/09/21/solrde-shu-zhi-cun-chu-he-fan-wei-cha-xun/"/>
    <updated>2012-09-21T20:49:00+08:00</updated>
    <id>http://www.wanghd.com/blog/2012/09/21/solrde-shu-zhi-cun-chu-he-fan-wei-cha-xun</id>
    <content type="html"><![CDATA[<h3>问题：solr查询q=date:[1348233109 TO *]，QTime=300-500毫秒，范围越大越耗时</h3>

<h3>要点：lucene内部不存储数值，而用多个字符串代替</h3>

<h4>我的疑问：为什么不用数值呀，存储空间少，效率又高? 琢磨了下应该有如下原因吧</h4>

<ul>
<li>lucene索引的倒排表都是根据字符串排序的，要存储数值看来得另起一套索引</li>
<li>如果存储数值，查询的排序也不能正常运作了，比如10比2排在前面</li>
<li>查询12,是准确匹配12能还是能匹配123</li>
<li>数字问题越来越复杂，基本上串模式在数字上根本行不通，如果数值也是串，那问题就简单多了,和一般的词没两样</li>
</ul>


<h3>lucene测试程序</h3>

<ul>
<li>添加id为695和705的两个文档，搜索id区域698到2054</li>
</ul>


<p>```java
public static void main(String[] args) throws IOException, ParseException{</p>

<pre><code>Directory dir = FSDirectory.open(new File("/tmp/tempindex"));
Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_40);
IndexWriterConfig iwc = new IndexWriterConfig(Version.LUCENE_40, analyzer);
iwc.setOpenMode(OpenMode.CREATE);
IndexWriter writer = new IndexWriter(dir, iwc);
Document doc = new Document();  
doc.add(new LongField("id",695,Store.YES));  
writer.addDocument(doc);  
doc = new Document();  
doc.add(new LongField("id",705,Store.YES));  
writer.addDocument(doc);  
writer.close();

DirectoryReader reader = DirectoryReader.open(dir);
IndexSearcher searcher = new IndexSearcher(reader);
Query q = NumericRangeQuery.newLongRange("id", 698L, 2054L, true, true); 
ScoreDoc[] hits = searcher.search(q, 200).scoreDocs;  
for(int i=0; i &lt; hits.length; i++){
  Document result = searcher.doc(hits[i].doc);
  IndexableField id = result.getField("id");
  System.out.println(String.format("doc:%s,id:%s", hits[i].doc, id.numericValue().longValue()));
}
reader.close();
dir.close();
</code></pre>

<p>}
```</p>

<h3>调试监测建立索引时，695和705生成的多个字符串</h3>

<ul>
<li>断点设在NumericUtils.java的longToPrefixCoded方法内</li>
</ul>


<p>```</p>

<pre><code>      695                         705
</code></pre>

<p>[20 1 0 0 0 0 0 0 0 5 37]   [20 1 0 0 0 0 0 0 0 5 41] 0x20代表long,低字节放到bytes高索引位<br/>
[24 8 0 0 0 0 0 0 0 2b]     [24 8 0 0 0 0 0 0 0 2c]   每7位代表一个字符
[28 40 0 0 0 0 0 0 2]       [28 40 0 0 0 0 0 0 2]     每次循环右移precisionStep=4
[2c 4 0 0 0 0 0 0 0]        [2c 4 0 0 0 0 0 0 0]
[30 20 0 0 0 0 0 0]         [30 20 0 0 0 0 0 0]
[34 2 0 0 0 0 0 0]          [34 2 0 0 0 0 0 0]
[38 10 0 0 0 0 0]           [38 10 0 0 0 0 0]
[3c 1 0 0 0 0 0]            [3c 1 0 0 0 0 0]
[40 8 0 0 0 0]              [40 8 0 0 0 0]
[44 40 0 0 0]               [44 40 0 0 0]
[48 4 0 0 0]                [48 4 0 0 0]
[4c 20 0 0]                 [4c 20 0 0]
[50 2 0 0]                  [50 2 0 0]
[54 10 0]                   [54 10 0]
[58 1 0]                    [58 1 0]
[5c 8]                      [5c 8]
```</p>

<h3>698-2057的区域转化</h3>

<ul>
<li>断点设在NumericUtils.java的longToPrefixCoded方法内</li>
</ul>


<p><code>
698: [20 1 0 0 0 0 0 0 0 5 3a]  703: [20 1 0 0 0 0 0 0 0 5 3f]
704: [24 8 0 0 0 0 0 0 0 2c]    767: [24 8 0 0 0 0 0 0 0 2f]
768: [28 40 0 0 0 0 0 0 3]      2047:[28 40 0 0 0 0 0 0 7]
2048:[20 1 0 0 0 0 0 0 0 10 0]  2054:[20 1 0 0 0 0 0 0 0 10 6]
</code>
+ precisionStep使用默认的4
+ 703的生成方法是对698的右边4位和二进制“1111”做或运算
+ 704的生成方法是在703的基础上加1,那么右边四位全0，右移4位
+ 767的生成方法是对704的右边4位和二进制“1111”做或运算（其实只要前缀匹配，后面已经无所谓了）
+ 后面的数字生成方法一样</p>

<h3>查询索引比较</h3>

<ul>
<li>大于等于698,小于等于703的没有</li>
<li>大于等于704,小于等于767的有，705的第二条记录</li>
<li>大于等于768,小于等于2047的没有</li>
<li>大于等于2048,小于等于2054的没有</li>
</ul>


<h3>结论：要提高效率的方法</h3>

<ul>
<li>调整precisionStep,默认为4，调大索引小性能差，调小索引大性能理论上的好,精度高，见下图</li>
</ul>


<p><img src="/images/post/rangequery.gif" title="rangequery" alt="rangequery" /></p>

<ul>
<li>如果int够用，可以不用Long用Int</li>
<li>缩小范围，这是最直接最管用的方法，（优化业务需求）</li>
</ul>

]]></content>
  </entry>
  
</feed>
